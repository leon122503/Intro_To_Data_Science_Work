{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fafdc4-f5bc-419b-a3f5-811d7b247410",
   "metadata": {},
   "source": [
    "# INFO204 Practical Test 1 (Practice) - Model Answer\n",
    "\n",
    "## PLEASE NOTE: THE PURPOSE OF THIS PRACTICE TEST IS TO PROVIDE A _ROUGH_ IDEA OF HOW THE TEST WILL BE PRESENTED - WHILE WILL BE OVERLAP IN CONCEPTS, THE ORDER AND MANNER IN WHICH QUESTIONS ARE ASKED MAY DIFFER. ALSO, THE ACTUAL TEST WILL CONTAIN A SMALL NUMBER (2-3) OF EXAM-LIKE SHORT ANSWER QUESTIONS THAT ARE BASED ON LECTURE CONTENT, DETAILS ON HOW TO PREPARE FOR THESE QUESTIONS WILL BE PROVIDED ON BLACKBOARD\n",
    "\n",
    "Please enter your details below:<br />\n",
    "***Student Name:***\n",
    "\n",
    "***Student ID:***\n",
    "\n",
    "## Guidelines:\n",
    "- Attempt **all** tasks as best you can.\n",
    "- Type in your solution code in the cell *right under* each task and run it. Keep the output. \n",
    "- If stuck on one task, don't waste your time - there are other tasks you can attempt.\n",
    "- All work (bar one exercise) has been discussed in labs - please use the previous labs as inspiration to complete this test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1e64e-e021-4595-928e-376d069c7006",
   "metadata": {},
   "source": [
    "## Precursors\n",
    "\n",
    "### <span style=\"color: #ce2227;\">Please run the first cell to import relevant libraries and to define the `extract_cv_stats` function that will be used later in the test. This will also declare a repeated 10-fold cross validation generator called `rkf` and a mean squared error scoring function `mse_score` that will be used throughout the test.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930c0f42-4a2c-4e77-9c0e-5d568ccacabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def cleanup_cv_results(cv, model_name='model'):\n",
    "    import re\n",
    "    \n",
    "    cv_results = pd.DataFrame(cv.cv_results_)\n",
    "\n",
    "    ## there's a few columns returned by GridSearchCV.fit() that we don't need, so let's get rid of them to make things clearer\n",
    "    unwanted_columns = ['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'mean_test_score', 'std_test_score', 'rank_test_score']\n",
    "\n",
    "    ## remove the \"param_\"  and \"param_model__\" prefixes from columns\n",
    "    r = re.compile(f\"param_({model_name}__)*\")\n",
    "    cleaned_names = cv_results.drop(columns=unwanted_columns).rename(columns=lambda x: r.sub('', x))\n",
    "\n",
    "    ## identify all the columns that are not the per-split cross validation scores\n",
    "    r = re.compile(f\"split.+_test_score\")\n",
    "    header_cols = [ c for c in cleaned_names.columns.values if not r.match(c) ]\n",
    "    \n",
    "    ## return the long version of the data\n",
    "    long_data = cleaned_names.melt(id_vars=header_cols, var_name='split', value_name='score')\n",
    "    long_data['split'] = long_data['split'].replace('split([0-9]+)_test_score', '\\\\1', regex=True).astype(int)\n",
    "    return long_data\n",
    "\n",
    "rkf = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acf2ee-f1ff-4e34-b0bf-ea2af231619f",
   "metadata": {},
   "source": [
    "## Data Manipulation\n",
    "\n",
    "***Note: <span style=\"color: #ce2227;\">In this test, assume the response variable is named `target` and use this as the response (target) in all subsquent questions</span>***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d628969-924b-44e2-99ca-20edbfa825e0",
   "metadata": {},
   "source": [
    "### Q1: load the CSV file <span style=\"font-family: monospace\">regression_features.csv</span> into a pandas data frame called `features`. Then, load the CSV file <span style=\"font-family: monospace\">regression_target.csv</span> into a pandas data frame called `target`. Join these two data frames together using the common column information - name the resulting data frame `all_data`. Display the data frame `all_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ccb4ff-ef3f-4f5e-b583-e9357289606c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'regression_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leon1\\INFO204\\info204_practice_practical_test_solution.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/leon1/INFO204/info204_practice_practical_test_solution.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mregression_features.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leon1/INFO204/info204_practice_practical_test_solution.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m target \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mregression_target.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leon1/INFO204/info204_practice_practical_test_solution.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m all_data \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mmerge(features, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minner\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minstance\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\leon1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\leon1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\leon1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\leon1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\leon1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m     f,\n\u001b[0;32m   1220\u001b[0m     mode,\n\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\leon1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'regression_features.csv'"
     ]
    }
   ],
   "source": [
    "features = pd.read_csv('regression_features.csv')\n",
    "target = pd.read_csv('regression_target.csv')\n",
    "\n",
    "all_data = target.merge(features, how='inner', on='instance')\n",
    "display(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53609ec2-059b-4b38-8b7b-069e3149ce60",
   "metadata": {},
   "source": [
    "### Q2: `all_data` is in long format - convert it to wide format, using the `instance` and `target` columns as index. Once the data frame is in wide format, use the [`reset_index`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html) function to restore `instance` and `target` as actual columns. Then, remove the `instance` column from the data frame. Finally, display the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a4aac9-9277-473a-a8d6-2870e7ab0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.pivot(index=[ 'instance', 'target' ], columns='feature', values='value').reset_index()\n",
    "all_data.drop(columns='instance', inplace=True)\n",
    "display(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5efd7-2f60-4560-9504-26eebbd5bec6",
   "metadata": {},
   "source": [
    "### Q3: Save the edited wide-format data frame to a new CSV file called <span style=\"font-family: monospace\">tidy_regression_data.csv</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488efa4-b9ec-4e6c-b2f3-bb9a72996f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('tidy_regression_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c323946-3cee-4cb4-adb7-46e1c6098dfd",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741a4cd-d819-4ccc-be47-e427d011d32f",
   "metadata": {},
   "source": [
    "### Q4: Read the CSV file <span style=\"font-family: monospace\">regression_data.csv</span> into a data frame called `dataset`. Then, print out descriptive statistics of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1f0e0-e460-4266-9dc7-b0465bada1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('regression_data.csv')\n",
    "\n",
    "display(dataset)\n",
    "\n",
    "display(dataset.info())\n",
    "\n",
    "display(dataset.describe())\n",
    "\n",
    "display(dataset.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a32bc-8cf0-49dc-a866-e5d38bbd3823",
   "metadata": {},
   "source": [
    "### Q5: Produce a heatmap of the correlations of the numeric columns in `dataset`, and a pairwise scatter plot of `dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6360a-7c5d-4b84-945e-5e5293046241",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.plotting_context(rc={ 'axes.labelsize' : 20, 'xtick.labelsize' : 10, 'ytick.labelsize' : 10 }):\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    sns.heatmap(dataset.corr())\n",
    "\n",
    "    sns.pairplot(dataset, aspect=1, height=3, corner=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e913dd-e703-4bd2-b946-c79a23d403b7",
   "metadata": {},
   "source": [
    "### Q6: Identify THREE columns in `dataset` for removal and briefly suggest (1-3 sentences total) why these columns can be removed. Then, put the names of the columns into a list called `drop_columns`. Finally, remove these columns from `dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbe549-5a16-4683-b5f3-108aff48c315",
   "metadata": {},
   "source": [
    "#### _a and c appear to have little to no relationship with the response. While d and e appear to be uncorrelated to the response, they have a clear underlying non-linear relationship to the response (and so a non-linear learner may be able to exploit them). Finally, f has MANY missing values, so rather than remove instances with missing values, we will remove this column_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001ce20-8fca-4727-a3e2-8d6cbe25de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [ 'a', 'c', 'f' ]\n",
    "dataset.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e4593-8bfc-454f-bd6f-5598a6b356ae",
   "metadata": {},
   "source": [
    "### Q7: Extract the features of `dataset` into an array named `X`, and the response into a variable named `t`. Create a list of the feature names, and store the result in a variable named `feature_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf19814-3f47-4575-b1d5-56701f999a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'target'\n",
    "X = dataset.drop(columns=target).to_numpy()\n",
    "t = dataset[target].to_numpy()\n",
    "feature_names = dataset.drop(columns=target).columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35018285",
   "metadata": {},
   "source": [
    "### Q8: define a machine learning pipeline named `mlpipe`. This pipeline needs two steps: one called `'preprocess'` that is set to `'passthrough'` by default, and another step called `'model'` that defaults to a `DummyRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpipe = Pipeline([\n",
    "    ('preprocess', 'passthrough'),\n",
    "    ('model', DummyRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d7787-90c8-4859-bc33-5ee8178c3c21",
   "metadata": {},
   "source": [
    "### Q9: define a suitable hyperparameter tuning grid for the pipeline `mlpipe` such that it uses a decision tree to model the data. The tuning grid should explore the following decision tree hyperparameter `min_samples_split` at the values: 2, 4, 8, 16, 32, 64, 128, and 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914ca80-3105-4372-96bd-6841b1c5cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CART = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "CART_param_grid = {\n",
    "    'preprocess' : [ 'passthrough' ],\n",
    "    'model' : [ CART ],\n",
    "    'model__min_samples_split' : np.logspace(np.log2(2), np.log2(256), 8, base=2).astype(int)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40d7787-90c8-4859-bc33-5ee8178c3c21",
   "metadata": {},
   "source": [
    "### Q10: define a suitable hyperparameter tuning grid for the pipeline `mlpipe` such that it uses k-nearest neighbours to model the data. The preprocessing step should explore standardisation (i.e., using `'passthrough'` or a `StandardScaler`). The tuning grid should also explore the k-nearest neighbour hyperparameters:\n",
    "1. `n_neighbors` at the values: 2, 4, 8, 16, 32, 64, 128, and 256.\n",
    "2. `weights` at the values 'uniform', 'distance'\n",
    "\n",
    "Note: we have not discussed the `weights` hyperparameter for kNN in classes - <strong>you do not need to know what this hyperpameter does for kNN, you only need to be able to assess its impact on performance of kNN for the given problem</strong>. The `weights` hyperparameter adjusts the \"Weight function used in prediction\" (in other words, how the neighbouring instances are combined to form the final prediction). Possible values are:\n",
    "* 'uniform' : uniform weights. All points in each neighborhood are weighted equally. (this is what you've been using all semester)\n",
    "* 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3914ca80-3105-4372-96bd-6841b1c5cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "knn_param_grid = {\n",
    "    'preprocess' : [ 'passthrough', scaler ],\n",
    "    'model' : [ knn ],\n",
    "    'model__n_neighbors' : np.logspace(np.log2(1), np.log2(256), 9, base=2).astype(int),\n",
    "    'model__weights' : [ 'uniform', 'distance' ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f3527",
   "metadata": {},
   "source": [
    "### Q11: Perform cross validation on the tuning grids defined in the previous two questions. Extract the best model identified by cross validation and assign it to a variable named `best_model`. Describe the hyperparameters and score of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd3ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    CART_param_grid,\n",
    "    knn_param_grid\n",
    "]\n",
    "\n",
    "cv = GridSearchCV(mlpipe, param_grid, cv=rkf)\n",
    "cv.fit(X, t)\n",
    "\n",
    "best_model = cv.best_estimator_\n",
    "\n",
    "print(cv.best_params_)\n",
    "print(cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26cc97",
   "metadata": {},
   "source": [
    "#### _DESCRIBE THE MODEL PARAMETERS AND SCORE HERE_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc35fe-f819-46e3-a290-e91a0d15d7c5",
   "metadata": {},
   "source": [
    "### Q12: Use the provided `cleanup_cv_results` to extract a data frame called `cv_stats`. Then use `cv_stats` to create two line plots: one with the x axis exploring the `min_samples_split` hyperparameter of the decision tree, and another with the x axis exploring the `n_neighbors` hyperparameter of kNN. For the kNN plot, use the `hue` semantic to visualise the `'weights'` hyperparameter, and a `style` hyperparameter to visualise the `'preprocess'` hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70681845-eec0-4eae-b5c8-5c00ff1681a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_stats = cleanup_cv_results(cv)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(18, 6))\n",
    "sns.lineplot(data=cv_stats, x='min_samples_split', y='score', color='black', ax=axs[0]).set(title='CART', xscale='log', xlabel='$minsplit$')\n",
    "sns.lineplot(data=cv_stats, x='n_neighbors', y='score', hue='weights', style='preprocess', ax=axs[1]).set(title='$k$-NN', xscale='log', xlabel='$k$ (Neighbourhood Size)')\n",
    "for ax in axs: ax.set_ylim(bottom=-0.1, top=1.0)\n",
    "for ax in axs: ax.set_ylabel('$R^2$')\n",
    "fig.suptitle('Cross Validation Grid Search Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ea908-e177-4102-a84e-b3e5ccf1862f",
   "metadata": {},
   "source": [
    "### Q13: declare a new scorer called `mse_score` that uses `mean_squared_error` (in other words, use the scikit-learn function `make_scorer` to make a new scorer based on the `mean_squared_error` loss function). Perform cross validation on linear regression (using `X` and `t` as source data) using `mse_score` as the required scorer and store the result of this in `lm_scores`. Then, perform cross validation on the best model returned in Q11 (using `X` and `t` as source data) using `mse_score` as the required scorer and store the result of this in `best_scores`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cd75b7-3c43-4270-871e-eb6fe6629507",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_scorer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leon1\\INFO204\\info204_practice_practical_test_solution.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/leon1/INFO204/info204_practice_practical_test_solution.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mse_score \u001b[39m=\u001b[39m make_scorer(mean_squared_error)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leon1/INFO204/info204_practice_practical_test_solution.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lm_scores \u001b[39m=\u001b[39m cross_val_score(LinearRegression(), X, t, cv\u001b[39m=\u001b[39mrkf, scoring\u001b[39m=\u001b[39mmse_score)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leon1/INFO204/info204_practice_practical_test_solution.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m best_scores \u001b[39m=\u001b[39m cross_val_score(best_model, t, t, cv\u001b[39m=\u001b[39mrkf, scoring\u001b[39m=\u001b[39mmse_score)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_scorer' is not defined"
     ]
    }
   ],
   "source": [
    "mse_score = make_scorer(mean_squared_error)\n",
    "\n",
    "lm_scores = cross_val_score(LinearRegression(), X, t, cv=rkf, scoring=mse_score)\n",
    "best_scores = cross_val_score(best_model, X, t, cv=rkf, scoring=mse_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bfb9c-f9b6-45d0-9b9f-215fecbb94a3",
   "metadata": {},
   "source": [
    "### Q14: Create a data frame, called `results`, that contains two columns: the `best_scores` and `lm_scores` arrays from Q13. Convert this data frame from its wide format into long format, naming the \"variable\" column `'method'` and the \"value\" column `'MSE'`. Group this long version of the data frame by `'method'` and aggregate the result such that the resulting data frame presents the mean MSE value for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f2377-831c-4e04-9c07-087ea7bb1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Best from Grid Search' : best_scores,\n",
    "    'Linear Regression'   : lm_scores\n",
    "}).melt(var_name='method', value_name='MSE')\n",
    "\n",
    "display(results.groupby('method')[['MSE']].mean().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b64ab",
   "metadata": {},
   "source": [
    "### Q15: Use the `results` data frame to create a boxplot comparing the cross-validated mean-squared error results obtained in Q13. Briefly discuss the relative performance of the methods, and declare which method you would select as your final model for future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "sns.boxplot(data=results, x='method', y='MSE', ax=ax)\n",
    "ax.set_ylabel('MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c2430-978b-4b7f-a898-7749943fdee8",
   "metadata": {},
   "source": [
    "#### _Metric is MSE, so we aim to minimise. Linear regression can be considered a reasonable baseline of expected performance, and we can see that the model discovered through cross validation (standardised kNN, with a neighbourhood size of 4 and using distance weighting) is a substantial improvement over this._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2107224-9360-41c3-83a0-092048c55124",
   "metadata": {},
   "source": [
    "**SELECTED MODEL:** The one returned by cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8c873-2da3-42ff-9b0b-9734518a5627",
   "metadata": {},
   "source": [
    "### Q16: load the CSV file <span style=\"font-family: monospace\">new_regression_data.csv</span> into a pandas data frame. Name the data frame `test`. Use `drop_columns` to remove the extraneous columns (as identified in Q6) in `test`. Extract the features in the `test` data frame into an array called `X_test` and the response into a variable called `t_test`. Use the selected model identified in the previous question to obtain predictions from `X_test` and store the result in `y_test`. Finally, compute and print out the mean squared error and $R^2$ of the predictions in `y_test` relative to `t_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0022b060-765e-40a6-9127-722af788b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('new_regression_data.csv')\n",
    "test.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "X_test = test.drop(columns=target).to_numpy()\n",
    "t_test = test[target].to_numpy()\n",
    "y_test = cv.best_estimator_.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(t_test, y_test), r2_score(t_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ce7cca635952bebe4ff973d82e24d837bdff99cd5a17557994c0e84479824bc0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
